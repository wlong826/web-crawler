{"cells":[{"metadata":{"id":"54354D8F335A481CAB48DDB3B7ED1FC0","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 零基础入门Python爬虫系列\n## 第二章 反爬虫手段之User-Agent，Cookie，Referer，time.sleep()，IP代理池"},{"metadata":{"id":"272A54ADECD4413DA51889886AEDD33B","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　ID：wlong\n　数据分析爱好者、目前小白一枚\n　邮箱：wlong826@163.com\n　CSDN：【[数分小白龙](https://blog.csdn.net/qq_38230663?spm=1001.2014.3001.5343)】\n　如果有不完善的地方，欢迎小伙伴评论区留言！\n　最近博客、和鲸每周都会有所更新，欢迎大家**关注点赞分享**！\n\n------\n\n 　本文的CSDN链接：[Python之反爬虫手段（User-Agent，Cookie，Referer，time.sleep()，IP代理池）](https://blog.csdn.net/qq_38230663/article/details/116830990)\n"},{"metadata":{"id":"F5008BB1CC0C4639B430EFEE83EFCD3E","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　　现在的爬虫越来越难，各大网站为了预防不间断的网络爬虫，都相应地做出了不同的反爬机制，那么如何能够在不被封IP的情况，尽可能多得爬取数据呢？这里主要介绍到一些通用的反爬措施，虽然不一定适合所有网站，但是大部分网站的爬取，个人认为还是可以的。本文主要介绍到**User-Agent，Cookie，Referer，time.sleep()设置睡眠间隔，ProxyPool之IP池的搭建**，小伙伴们各取所需！\n\n　　由于后续爬虫案例都默认自带这些反爬技术，所以这里就统一详细介绍下，后续案例就不再过多涉及，废话不多说，开始展开！"},{"metadata":{"id":"018D42D8365E43D484C5706F76FA98CA","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 1. user-agent\n　　**user-agent**：是识别浏览器的一串字符串，相当于浏览器的身份证，在利用爬虫爬取网站数据时，频繁更换User-agent可以避免触发相应的反爬机制；\n　　\n　　这里，就用到了**fake-useragent**包，这个包对频繁更换User-agent提供了很好的支持，可谓防反爬利器，那么它如何使用呢？"},{"metadata":{"id":"8E6B1CB737D9482A8E93BAF9B9EEB28B","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （1）安装：\n　　直接在anaconda控制台安装fake-useragent包即可；\n\t\n　　`pip install fake-useragent`\n　　"},{"metadata":{"id":"C6A05B5C09BC4B55BA8041DBE5FE3C57","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （2）具体代码使用"},{"metadata":{"id":"4398471AE1FB47418E48B181BF178080","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from fake-useragent import UserAgent\nua = UserAgent()\nheaders = {\n    'User-Agent' : ua.random #随机生成一个UserAgent\n}\nurl = 'https://www.baidu.com/'\npage = requests.get(url, headers=headers)","execution_count":3},{"metadata":{"id":"336DCCD13AB941C29C4219B4F2DBF44E","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 2. cookie，referer设置\n## （1）Cookie\n\n　　在我们每次访问网站服务器的时候，服务器都会为在我们本地设置cookie，为什么要设置cookie呢？因为服务器要了解我们的身份。在我们下一次访问该服务器的时候，都会带上这个cookie，表明我们的身份。（例如我们在登陆某个网站的时候，在一段时候内再次进行访问，就不需要二次登录）\n\t\n　　那么，每个网站的Cookie都不一样，如何找到自己需要的呢？\n\t\n　　首先，打开一个特定的网站，比如[58同城二手房](https://bj.58.com/ershoufang/)在网页空白处右键（最好是chrome或者Firefox），点击【检查】，出现如下图所示的框（右边即为浏览器检查界面）；\n\t![Image Name](https://img-blog.csdnimg.cn/20210515093211372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)"},{"metadata":{"id":"615E5386A8B940EC8B38E6E2DA8346AD","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　　4步走：① 选择【Network】②选择【XHR】③点击浏览器刷新按钮④找到对应的网站请求，点击打开即可。\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515094006795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n![Image Name](https://img-blog.csdnimg.cn/20210515094330561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)"},{"metadata":{"id":"572DA37B1EDF432996B045E470DE12E9","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （2）Referer\n\n　　这个请求参数的作用主要是标识请求是从哪个页面过来的。\n\t\n　　例如：在登陆某个网站的时候，登陆成功会跳转到个人中心。那么Referer的值就会是登录界面的url。应用场景：来源统计，防盗链处理等；Referer的获取同Cookie，具体如下图所示；个人感觉也可以统一将**第一页url**作为referer!\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515094814576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n"},{"metadata":{"id":"C82D9E09DE9841F48A20C5C3570C5DA0","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 3. time.sleep()：设置访问时间间隔\n　　很多网站的反爬虫机制都设置了访问间隔时间，一个IP如果短时间内超过了指定的次数就会进入“冷却CD”，所以除了轮换IP和user_agent，可以设置访问时间间隔，让间隔长一些，比如每抓取一个页面休眠一个随机时间：\n\n　　相对来说，这是一个比较可靠的做法。 因为本来爬虫就可能会给对方网站造成访问的负载压力，所以这种防范既可以从一定程度上防止被封，还可以降低网站的访问压力。如果访问过于频繁，有些网站会直接封掉IP，让你再也无法访问其数据。所以为了保险起见，最好设置下睡眠时间。\n\n　　那么如何设置访问时间间隔呢？代码很简单，只需在爬取时加到循环里就可以。"},{"metadata":{"id":"09EED6AACC924E03B61811920775C13B","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"import time\nimport random\ntime.sleep(random.randint(5,10)) \n#具体的随机数，可以自行设置，太小的话，作用不大，太大的话，代码运行时间会加长;","execution_count":null},{"metadata":{"id":"AE76B3E73D384AAFB39719C7058D802D","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 4. ProxyPool之IP池的搭建\n　　如果一个固定的IP在短暂时间内，快速大量的访问一个网站，那自然会引起管理员注意，于是管理员通过一些手段就会把这个IP给封了，让你爬不到其网站任何数据。目前，解决这个问题比较成熟的方式就是**使用IP代理池**。简单的说，就是通过IP代理，用不同的IP进行访问，这样在一定程度上就避免封掉IP的风险了。可是IP代理的获取比较麻烦，网上有免费和付费的，但是质量都层次不齐。如果是企业里需要的话，可以通过自己购买集群云服务来自建代理池。那么对于个人户的我们，如何搭建IP代理池呢？\n\n　　前段时间，在查阅相关资料的时候，发现了一个很好用的IP代理池ProxyPool，关于ProxyPool使用的详细流程见下。"},{"metadata":{"id":"7CD09F960F9C4699844C8166E830A270","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （1）ProxyPool下载\n\n　　具体的Github下载地址：[ProxyPool](https://github.com/Python3WebSpider/ProxyPool.git)\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515101943964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　下载完解压到本地，用Pycharm打开；\n"},{"metadata":{"id":"1F57C796CD2442448B1F55097DC8D440","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （2）ProxyPool使用\n\n　　①  如果已安装redis数据库，按照下图所示，找到settings.py文件，修改PASSWORD为redis数据库的密码，如果为空，则设置为None。（新安装的redis数据库一般默认没有密码）\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515102400527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　② 如果未安装redis数据库，则需要先安装redis数据库，然后才能使用ProxyPool包搭建IP代理池。（Redis数据库安装见文末链接）"},{"metadata":{"id":"1B2C85183E874A3F8FE24A6C7C6E74D1","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （3）安装ProxyPool相关依赖包\n\n　　在已下载好的ProxyPool-master目录里，找到requirements.txt（很重要），在这个txt文件中，有很多ProxyPool的依赖的Python包。\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515103547650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)"},{"metadata":{"id":"1407C98EF4084F62825A162CB5FE0D56","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　　**安装方式：三种，经验所谈。**（当初在这出了不少错，花费不少时间安装依赖包）\n\n　　① 如果你的Pycharm是正版的，可以在pycharm直接安装这些包，非常方便；如果不能直接在Pycharm中安装，可以在anaconda或者python的控制台去安装；\n\n　　② 官网给出的安装方式：（这代码需要稍微更改下，当初这句代码一直运行不了，主要是这个txt当初没找到，找到后也没有运行成功，后来认真想了想，发现是路径的问题），下面这句代码需改成：**pip install -r 绝对路径/requirements.txt **     （其中，绝对路径根据自己所放的地方来，文件就在ProxyPool-master的目录里）；\n![Image Name](https://img-blog.csdnimg.cn/20210515104014815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　③ 如果前两种方法都不能运行的话，还有一种相对比较麻烦的方法，就是在anaconda控制台，将文件里的这些包通过**pip**一个个的安装，例如：**pip install environs=9.3.0**；"},{"metadata":{"id":"1DBDD6D1DDC04443803AA26C3F94296F","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （4）开启Redis服务器\n\n　　找到Redis的下载目录，直接双击打开**redis-server.exe**，出现如下图所示界面，即表示Redis服务器开启成功，Redis的默认端口为6379。\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515105438358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n\n![Image Name](https://img-blog.csdnimg.cn/20210515105815337.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n"},{"metadata":{"id":"F9ABF169A05A4C81B6ABFA9CF590F311","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （5）运行ProxyPool-master目录中的run.py文件\n\n　　在Pycharm中，找到ProxyPool-master项目目录下的run.py文件，然后运行该python文件；\n\t\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515110226805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　运行run.py后，**不要停止项目**，这时可以打开redis管理工具（**RedisDesktopManager**，这个管理工具也需要下载，下载后需连接到Redis数据库，**安装教程见文末链接**），从中能够查看爬取到的所有代理IP。\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515110857282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　ProxyPool-master项目运行过程中，千万**不要点击【停止运行】按钮**，如果项目停止了，就无法访问代理池了。那么如何访问并获取到Redis数据库存的这些IP呢？\n"},{"metadata":{"id":"1A90319390B049C3B8C645FFCFD35C78","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （6）访问并获取Redis数据库中的代理IP\n\n　　在项目刚开始运行时，我们可以看到运行窗口如下图。\n\t\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515111459948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n\n　　在上图中，我们可以看到有如下这么一行语句，这句话告诉我们随机访问地址URL是多少。\n　　　　Running on http://0.0.0.0:5555/ (Press CTRL+C to quit)\n\n　　我们在浏览器地址栏中，输入：http://0.0.0.0:5555/random\n\t\n\t　　出现错误，该网址无法访问！为什么出错呢？\n\t\t\n![Image Name](https://img-blog.csdnimg.cn/20210515112453498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)"},{"metadata":{"id":"D3004F76288D40688F0368A99371BE9D","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　　后来根据自己理解想了想，这个0.0.0.0可能与本地地址有关，于是我将0.0.0.0换成localhost试了试，网址如下。http://localhost:5555/random\n\t\n　　运行成功！随机获取到了Redis数据库中的一个IP，但是关于0.0.0.0无法访问，而localhost可以访问，具体原因我还是不太清楚，希望后续查阅相关资料，再来作详细说明，如果有小伙伴知道，可以在后续留言哦，一起学习。\n\t\n\t\n![Image Name](https://img-blog.csdnimg.cn/20210515112832605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMwNjYz,size_16,color_FFFFFF,t_70)\n"},{"metadata":{"id":"78524B2D6A2D4786BBB67A5EDC2EBD3D","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## （7）最终，如何将从IP代理池随机获取到的IP应用到爬虫程序中呢？\n\n　　以爬取安居客二手房小区名称为例，应用UserAgent，Cookie，referer，ProxyPool这些反爬手段，并作简要说明。\n\t\n　　（注：该案例只是为了应用反爬技术，至于更详细的步骤见后续爬虫案例。）"},{"metadata":{"id":"B585E06D101C452383649935B146B07F","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"# 以爬取安居客二手房小区名称为例\nimport requests\nfrom fake_useragent import UserAgent \nfrom lxml import etree\n \nua = UserAgent()\nheaders = {\n    \"user-agent\" : ua.random, #随机生成User-Agent；\n    \"cookie\" : \"xxx\"，# 根据自己浏览器自行获取，方法见上；\n    \"referer\" : \"https://www.anjuke.com/\", #设置从何网页跳转过来的\n}\n \n#从代理IP池中随机获取一个IP\ndef get_proxy():\n    try:\n        PROXY_POOL_URL = 'http://localhost:5555/random'\n        response = requests.get(PROXY_POOL_URL)\n        if response.status_code == 200:\n            return response.text\n    except ConnectionError:\n        return None\n \nurl = 'https://sjz.anjuke.com/community' # 以安居客二手房为例\ntext = requests.get(url=url, headers=headers, proxies={\"http\": \"http://{}\".format(get_proxy())}).text\nhtml = etree.HTML(text)\ncommunity_name = html.xpath('.//div[@class=\"comm-title\"]/h1/text()')\nprint(community_name)","execution_count":null},{"metadata":{"id":"629F1B133CDF41E98F3D264C7275C089","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"　　好了，到此有关反爬虫手段就讲解结束了，如果哪里有不完善或者出错的地方，欢迎大家帮忙指正，我会继续完善的！关于上文中未提到的**Redis数据库**以及**RedisDesktopManager管理工具**的下载与安装，这里就不详细去介绍了，因为我也是从各大博客中摸索的，这里就先把我找的资料放在这，大家各取所需哈，望体谅，如果后续关于这方面，自己有更深的理解，或许会写篇博客记录下。"},{"metadata":{"id":"9E82FB9F83CA4664A4E08DFE493680BC","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 参考资料\n**（1）Redis数据库**：\n\n　　[Redis的安装教程（Windows+Linux）【超详细】](\nhttps://blog.csdn.net/weixin_43883917/article/details/114632709)\n\n　　[本地redis服务安装](\nhttps://blog.csdn.net/qiucheng_198806/article/details/90479813)\n\n**（2）Redis Desktop Manager管理工具**\n\n　　[Redis Desktop Manager的下载及安装](\nhttps://www.jianshu.com/p/6895384d2b9e)\n\n**（3）关于安装ProxyPool参考资料**\n\n　　[教你自己搭建一个ip池(绝对超好用！！！！)](https://blog.csdn.net/weixin_44517301/article/details/103393145)\n\t\n　　[python爬虫添加代理ip池ProxyPool (Windows)](https://blog.csdn.net/qq_34442867/article/details/110817267)\n\t\n　　[python爬虫18 | 就算你被封了也能继续爬，使用IP代理池伪装你的IP地址，让IP飘一会](https://zhuanlan.zhihu.com/p/59951949)\n\n　　[如何使用 — ProxyPool 2.1.0 文档](https://proxy-pool.readthedocs.io/zh/latest/user/how_to_use.html)"},{"metadata":{"id":"01076C3287C1421F8B200FA625F25AC3","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 这一小节结束，后续就会开始各大网站的爬虫案例啦，内容较多，慢慢更新哈，奥里给，冲冲冲！\n\n-----\n\n![Image Name](https://cdn.kesci.com/upload/image/qudac6lar3.png?imageView2/0/w/960/h/960)"},{"metadata":{"id":"4577D4C317144CABA923C570B8377A12","notebookId":"60be29a4acdcb3001707b454","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}